# Master Theorem

1. Master theorem for solving recurrence relations.
2. The master method is a formula for solving recurrence relations of the forms:

```

T(n) = aT(n/b) + f(n).
where,
n    = size of input
a    = number of subproblems in the recursion
n/b  = size of each subproblem. All subproblems are assumed to have the same size.
f(n) = cost of the work done outside the recursive call, which includes the cost of dividing the problem and cost of merging  the solutions.

Here, a â‰¥ 1 and  b > 1 are constants, and f(n) is an asymptotically positive functions.

```


3. An asymptotically positive function means that for a sufficiently large value of **n**, we have **f(n)>0**.

```

Master theorem is used in calculating the time complexity of recurrence relations ( divide and conquer algorithms) in a simple and quick way.

```

 
### Master Theorem Explanation:

1. If **a â‰¥ 1** and **b > 1** are constants and **f(n)** is an asymptotically positive function, then the time complexity of a recursive relation is given by,

```
T(n) = aT(n/b) + f(n)
where, T(n) has the following asymptotic bounds:
    1. If f(n) = O(n^(logâ‚˜a-ð›†)), then T(n) = ð›‰(n^(log)â‚˜a).
    2. If f(n) = ð›‰(n^(logâ‚˜a)), then T(n) = ð›‰(n^(log)â‚˜a * log n).
    3. If f(n) = ð›€(n^(logâ‚˜a+ð›†), then T(n) = ð›‰(f(n)).
ð›† > 0 is a constant.
```
##### Each of the above conditions can be interpreted as:

1. If the cost of solving the sub-problems a each level increase by a a certain factor, the value  **f(n)** will become polynomially smaller than **n^(log)â‚˜a**.
2. Thus , the time complexity is oppressed by the cost of the last level ie. **n^(log)â‚˜a**
3. If the cost of solving the sub-problem at each level is nearly equal, then the value of **f(n)** will be **n^(log)â‚˜a**.
4. Thus, the time complexity will be **f(n)** time the total number of levels ie **n^(log)â‚˜a * log n**
5. If the cost of solving the subproblems at each level decreases by a certain factor, the value of **f(n)** will become polynomically larger than **n^(log)â‚˜a**.
6. Thus, the time complexity is oppressed by the cost of **f(n)**.

### Solved Examples of Master Theorem

```
T(n) = 3T(n/2) + n2
Here,
a = 3
n/b = n/2
f(n) = nÂ² 

logâ‚˜ a = logâ‚‚ 3 â‰ˆ 1.58 < 2

ie. f(n) < n^(logâ‚˜a+ð›†), where, ð›† is a constant.

Case 3 implies here.
Thus, T(n) = f(n) = ð›‰(nÂ²)


```

### Master Theorem Limitations


The master theorem cannot be use if:

1. T(n) is not monotone. **eg T(n) = sin n**.
2. f(n) is not a polynomial. **eg f(n) = 2n**
3. **a** is not a constant. **eg a = 2n**
4. **a < 1**



 
