# Why Learn Data Structures and Algorithms?

1. we will learn why every programmer should learn data structure and algorithms with help of examples.
2. This page is for those who have just started learning algorithms and wondered how impactful it will be to boost their career/programming skills.
3. It is also for those who wonder why big companies like Google,Facebook and Amazon hire programmers who are exceptionally good at optimizing Algorithms.
     
     
# What are Algorithms ?

1. Informally, an algorithm is nothing but a mention of steps to solve a problem.
2. They are essentially a solution.
3. For example, an algorithm to solve the problem of factorials might look something like this:
  
### Problems: Find the factorial of n
  
```
Initialize fact = 1
For every value v in range 1 to n:
    Multiply the fact by v
fact contains the factorial of n

```
 
1. Here ,the algorithm is written in English.
2. If it was written in a programming language, we would call it to code instead.
3. Here is a code for finding the factorial of a number in C++


```
int factorial(int n){
   int fact = 1;
   for (int v = 1; v <= n; v++){
     fact = fact * v;
   }
   return fact;
}

```

1. Programming is all about data structures and algorithms.
2. Data Structures are used to hold data while algorithms are used to solve the problem using that data.
3. Data structures and algorithms (DSA) goes through solutions to standard problems in detail and gives you an insight into how efficient it is to use each one of them.
4. It also teaches you the science of evaluting the efficieny of an algorithm.
5. This enables you to choose the best of various choices.

# Use of Data Structures and Algorithms to Make Your Code Scalable

### Time is precious

1. Suppose, Ironman and Spiderman are trying to solve a simple problem of finding the sum of the 10E11 natural numbers.
2. While Spiderman was writing the algorithm, Ironman implemented it proving that it is as simple as criticizing Super Hero's.

#### Algorithm (by Spiderman)

```
Initialize sum = 0
For every natural number n in range 1 to 1011 (inclusive):
   Add n to sum
sum is your answer.

```
#### Code (by Ironman)

```
int findsum(){
  int sum = 0;
  for(int v=1;v<=100000000000;v++){
     sum += v;
  }
  return sum;

}
```

3. Ironman and Spiderman are feeling euphoric of themselves that they could build something of their own in almost no time.
4. Let's sneak into their workspace and listen to their conversation.

```
Ironman: Let's run this code and find out the sum.
Spiderman: I ran this code a few minutes back but it's still not showing the output. What's wrong with it?
```

5. Oops, something went wrong! A computer is the most deterministic machine.
6. Going back and trying to run it again won't help.
7. So let's analyze what's wrong with this simple code.
8. Two of the most valuable resources for a computer program are **time** and **memory**.
9. The time taken by the computer to run code is:
```
Time to run code = number of instructions x time to execute each instruction 
```
10. The number of instruction depends on the code you used, and the time taken to execute each code depends on your machine and compiler.
11. In this case, the total number of instructions executed (let's say x) are

```
x=1 + (10ⁱⁱ + 1) + (10ⁱⁱ) + 1, 
which is x = 2 * 10 ⁱⁱ +3
```
12. Let us assume that a computer can execute y=10⁸ instructions in one second (it can vary subject to machine configuration).
13. The time taken to run above code is

```
Time taken to run code = x/y (greater than 16 minutes)
```

14. It is possible to optimize the algorithm so that Ironman and Spidermn do not have to wait for 16 minutes every time they run this code?
15. I am sure that you already guessed the right method. 
16. The sum of first **N** natural numbers is given by the formula

```
Sum = N * ( N+1 ) / 2
```
17. Converting it into code will look something like this:

```
int sum (int N){
 return N*(N+1)/2;
}
```
18. This code executes in just one instruction and gets the task done no matter what the value is.
19. Let it be greater than the total number of atoms in the universe.
20. It will find the result in no time.
21. The time taken to solve the problem, in this case, is **1/y** (which is 10 nanoseconds).
22. By the way, the fusion reaction of a hydrogen bomb takes 40-50 ns, which means your program will complete successfully even if someone throws a hydrogen bomb on your computer at the same time you ran your code.

```
Note: 1. Computers take a few instructions (not 1) to compute multiplication and division.
      2. I have said 1 just for the sake of simplicity.
```

# More on Scalability

1. Scalability is scale plus ability, which means the quality of an algorithm/system to handle the problem the larger size.
2. Consider the problem of setting up a classroom of 50 students.
3. One of the simplest solutions is to book a room, get a blackboard, a few chalks, and the problem is solved.

**But what if the size of the problem increases? What if the number of student increased to 200?**

1. The solution still holds but it needs more resources.
2. In this case, you will probably need a much larger room (probably a theater), a projector screen and a digital pen.

**What if the number of students increased to 1000?**

1. The solution fails or uses a lot of resources when the size of the problem increases.
2. This means, your solutions wasn't scalable.

**What is a scalable solution then?**
1. Consider a site like Facebook,Github,DockerHub,Youtube,Google Mail,Byju's,millions of students can see videos, read answers at the same time and on more resources are required.
2. So, the solution can solve the problems of larger size under resource crunch.

**Solution 1 :**
1. If you see our first solution to find the sum of first **N** natural number, it wasn't scalable.
2. It's because it required linear growth in time with the linear growth in the size of the problem.
3. Such algorithm are also known as linearly scalable algorithms.

**Solution 2 :**
1. Our second solution was very scalable and didn't required the use of any more time to solve a problem of larger size.
2. These are known as constant-time algorithms.

# Memory is expensive

1. Memory is not always available in abundance.
2. While dealing with code/system which requires you to store or produce a lot of data, it is critical for your algorithm to save the usage of memory wherever possible.
3. For example: while storing data about *people*, you can save memory by storing only their *age* not the date of birth.
4. You can always calculate it on the fly using their age and current date.

# Examples of an Algorithm's Efficiency:

1. Here are some examples of what learning algorithms and data structures enables you to do.

### Examples 1: Age Group Problem

1. Problems like finding the people of certain age group can easily be solved with a little modified version of the binary search algorithm (assuming that the data is sorted).
2. The native algorithm which goes through all the persons one by one, and checks if it falls in the given age group is linearly scalable.
3. Whereas, binary search claims itself to be a logarithmically scalable algorithm.
4. This means that if the size of the problem is squared, the time taken to solve it is only doubled.
5. Suppose, it takes 1 second to find all the people at a certain age for a group of 1000.
6. Then for a group of 1 million people,

```
1. The binary search algorithm will take only 2 seconds to solve the problem.
2. the native algorithm might take 1 millon seconds, which is around 12 days.
```
7. The same binary search algorithm is used to find the square root of a number.

### Example 2: Rubik's Cube Problem

1. Imagine you are writing a program to find the solution of a Rubik's cube.
2. This cute looking puzzle has annoyingly 43,252,003,274,489,856,000 positions, and these are just positions! Imagine the number of paths one can take to reach the wrong positions.
3. Fortunately, the way to solve this problem can be represented by the graph data structures.
4. There is a graph algorithm known as Dijkstra's algorithm which allow you to solve this problem in linear time.
5. Yes, you heard it right.
6. It means that it allows you to reach the solved position in a minimum number of states.

### Example 3: DNA Problem

1. DNA is a molecule that carries genetic information.
2. They are made up of smaller units which are represented by Roman characters A,C,T and G.
3. Imagine yourself working in the field of bioinformatics.
4. You are assigned the work of finding out the occurence of a particular pattern in a DNA strand.
5. It is a famous problem in computer science academia.
6. And, the simplest algorithm takes the time proportional to

```
(number of character in DNA strand) * (number of characters in pattern)
```

1. A typical DNA strand has millions of such units.
2. Eh! worry not.
3. **KMP(Knuth-Morris-Pratt) algorithm** can get this done in time which proportional to

```
(number of character in DNA strand) + (number of characters in pattern)
```
1. The * operator replaced by + makes a lot of change.
2. Considering that the pattern was of 100 characters, your algorithms is now 100 times faster.
3. If your pattern was of 1000 characters, the KMP algorithm would be almost 1000 times faster.
4. That is, if you were able to find the occurence of pattern in 1 second, it will now take you just 1ms.
5. We can also put this in another way.
6. Instead of matching 1 strand, you can match 1000 strands of similar length at the same time.
7. And there are infinite such stories....

# Final words 

1. Generally, software development involves learning new technologies on a daily basis.
2. You get to learn most of these technologies while using them in one of your projects.
3. However, it is not the case with algorithms.
4. If you don't know algorithms well, you won't be able to identify if you can optimize the code you are writing right now.
5. You are expected to know them in advance and apply them wherever possible and critical.
6. We specifically talked about the scalability of algorithms.
7. A software system consists of many such algorithms.
8. Optimizing any one of them leads to a better system.
9. However, it's important to note that this is not the only way to make a system scalable.
10. For example, a technique known as **distributed computing** allows independent parts of a program to run to multiple machine together making it even more scalable.
